{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN22+r3rpMsJGYlhio+vARy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedant-kharangkar/AI-CHATBOT-WITH-NLP/blob/main/AI_CHATBOT_WITH_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILD A CHATBOT USING NATURAL\n",
        "LANGUAGE PROCESSING LIBRARIES LIKE\n",
        "NLTK OR SPACY, CAPABLE OF ANSWERING\n",
        "USER QUERIES."
      ],
      "metadata": {
        "id": "BDda5Oedwuei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing neccesary  libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "4tvk66xQtyyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data set\n",
        "f=open('/content/data.txt','r',errors ='ignore')\n",
        "raw_doc = f.read()"
      ],
      "metadata": {
        "id": "p-oqRcg6t84y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_doc = raw_doc.lower() #converting entire text to lowercase\n",
        "nltk.download('punkt') # using the punkit tokenizer\n",
        "nltk.download('wordnet') # using the wordnet dictionary\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpOdor7muJHT",
        "outputId": "6776cc5e-defc-478e-d606-5da064b5f0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after tokenization\n",
        "sentence_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxJ15fZ_uVzU",
        "outputId": "b610c814-bb6d-4471-e306-2a9d32f235e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n\\nwikipediathe free encyclopedia\\nsearch wikipedia\\nsearch\\ndonate\\ncreate account\\nlog in\\n\\nhide\\nwle austria logo (no text).svgwiki loves earth photo contest:\\nupload photos of natural heritage sites in india to help wikipedia and win fantastic prizes!',\n",
              " 'contents hide\\n(top)\\nhistory\\n\\nturing test\\neliza\\nearly chatbots\\nmodern chatbots based on large language models\\napplication\\n\\nmessaging apps\\nas part of company apps and websites\\nchatbot sequences\\ncompany internal platforms\\ncustomer service\\nhealthcare\\npolitics\\ntoys\\nmalicious use\\ndata security\\nmental health\\nlimitations\\nimpact on jobs\\nimpact on the environment\\nsee also\\nreferences\\nfurther reading\\nexternal links\\nchatbot\\n\\narticle\\ntalk\\nread\\nedit\\nview history\\n\\ntools\\nappearance hide\\ntext\\n\\nsmall\\n\\nstandard\\n\\nlarge\\nwidth\\n\\nstandard\\n\\nwide\\ncolor (beta)\\n\\nautomatic\\n\\nlight\\n\\ndark\\nfrom wikipedia, the free encyclopedia\\nfor the bot-creation software, see chatbot.',\n",
              " 'for bots on internet relay chat, see irc bot.',\n",
              " 'a virtual assistant chatbot\\n\\nthe 1966 eliza chatbot\\npart of a series on\\nmachine learning\\nand data mining\\nparadigms\\nproblems\\nsupervised learning\\n(classification â€¢ regression)\\nclustering\\ndimensionality reduction\\nstructured prediction\\nanomaly detection\\nneural networks\\nreinforcement learning\\nlearning with humans\\nmodel diagnostics\\nmathematical foundations\\njournals and conferences\\nrelated articles\\nvte\\na chatbot (originally chatterbot)[1] is a software application or web interface designed to have textual or spoken conversations.',\n",
              " '[2][3][4] modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-_RtpJMuWj4",
        "outputId": "6eb155d6-bbe7-4f59-9394-dba2423980a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wikipediathe', 'free', 'encyclopedia', 'search', 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# performing text preprocessing steps\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def lemtokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "def lemnormalize(text):\n",
        "  return lemtokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "tMunLVCrukxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define greeting function\n",
        "greet_inputs = (\"hello\",\"hi\",\"whassup\",\"how are you?\")\n",
        "greet_responses = (\"hi\",\"hey\",\"hey there!\",\"there there!!\")\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "BXJ9_UwfuyhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response generation by bot\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "zWKqt26Au3BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response = ''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=lemnormalize,stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if req_tfidf == 0:\n",
        "    robo1_response = robo1_response + \"I am sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response + sentence_tokens[idx]\n",
        "    return robo1_response\n"
      ],
      "metadata": {
        "id": "YtDah7pju7mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print('hello! i am the learning bot. start typing your text after greeting to talk to me.for ending convo type bye!')\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if (user_response != 'bye'):\n",
        "    if(user_response == 'thanks' or user_response == 'thank you'):\n",
        "      flag = False\n",
        "      print(\"bot: you are welcome..\")\n",
        "    else:\n",
        "      if(greet(user_response) != None):\n",
        "        print(\"bot: \"+greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print('bot: ',end='')\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print('bot: goodbye!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovshjorgvCpq",
        "outputId": "e7464ab0-10cb-43e5-92b4-7b53b0629438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello! i am the learning bot. start typing your text after greeting to talk to me.for ending convo type bye!\n",
            "hi\n",
            "bot: hey\n",
            "tell me about Turing test\n",
            "bot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8]\n",
            "\n",
            "history\n",
            "turing test\n",
            "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published,[10] which proposed what is now called the turing test as a criterion of intelligence.\n",
            "tell me bout Application of network\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bot: machine learning with applications.\n",
            "bye\n",
            "bot: goodbye!\n"
          ]
        }
      ]
    }
  ]
}